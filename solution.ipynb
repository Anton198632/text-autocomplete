{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73390bf5",
   "metadata": {},
   "source": [
    "### 1. Разбиваем файл с твитами на тренировочную, валидационную и тестовую выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0caa40b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обработка текста...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600498/1600498 [00:12<00:00, 124908.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Перемешиваем данные...\n",
      "Размер тренировочного набора: 1280398 ( 80.0%)\n",
      "Размер валидационного набора: 160050 ( 10.0%)\n",
      "Размер тестового набора: 160050 ( 10.0%)\n",
      "\n",
      "Сохраняем разделенные датасеты...\n",
      "Тренировочный набор сохранен в: ./data/train.csv\n",
      "Валидационный набор сохранен в: ./data/val.csv\n",
      "Тестовый набор сохранен в: ./data/test.csv\n"
     ]
    }
   ],
   "source": [
    "from src.data_utils import process_text\n",
    "\n",
    "process_text('./data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13162eff",
   "metadata": {},
   "source": [
    "### 2. Тренируем собственную модель\n",
    "\n",
    "Для начала импортируем все необходимые модули"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebc7ccb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from src.data_utils import split_text_3_4\n",
    "from src.eval_lstm import generate_and_evaluate\n",
    "from src.lstm_model import SimpleLSTM\n",
    "from src.lstm_train import train_model_with_rouge\n",
    "from src.next_token_dataset import create_dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c57974",
   "metadata": {},
   "source": [
    "Загрузим датасеты и создадим даталоадеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6fda69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка токенизатора...\n",
      "Загрузка тренировочного датасета...\n",
      "Загрузка валидационного датасета...\n",
      "Загрузка тестового датасета...\n",
      "Создание даталоадеров...\n",
      "Количество батчей в dataloader: 5002\n",
      "\n",
      "Пример батча:\n",
      "input_ids shape: torch.Size([256, 38])\n",
      "attention_mask shape: torch.Size([256, 38])\n",
      "labels shape: torch.Size([256, 38])\n",
      "\n",
      "Пример данных (первые 2 элемента батча):\n",
      "\n",
      "Элемент 0: \n",
      "Вход: i feel like i was punched in the\n",
      "Цель: feel like i was punched in the arm\n",
      "\n",
      "Элемент 1: \n",
      "Вход: off to work http blip fm\n",
      "Цель: to work http blip fm hz\n",
      "Количество батчей в dataloader: 626\n",
      "\n",
      "Пример батча:\n",
      "input_ids shape: torch.Size([256, 41])\n",
      "attention_mask shape: torch.Size([256, 41])\n",
      "labels shape: torch.Size([256, 41])\n",
      "\n",
      "Пример данных (первые 2 элемента батча):\n",
      "\n",
      "Элемент 0: \n",
      "Вход: movie hmmm no its not true sorry u r\n",
      "Цель: hmmm no its not true sorry u r wrong\n",
      "\n",
      "Элемент 1: \n",
      "Вход: i never update twitter anymore so sad one week of summer\n",
      "Цель: never update twitter anymore so sad one week of summer left\n",
      "Используемое устройство: cpu\n"
     ]
    }
   ],
   "source": [
    "data_dir = './data/'\n",
    "\n",
    "# Создаем директорию для чекпоинтов\n",
    "save_directory = \"checkpoints\"\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# Загружаем токенизатор\n",
    "print('Загрузка токенизатора...')\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Добавляем специальные токены для начала/конца\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "print('Загрузка тренировочного датасета...')\n",
    "train_data = pd.read_csv(f'{data_dir}train.csv')\n",
    "train_texts = (\n",
    "    train_data['tweet'] if 'tweet' in train_data.columns\n",
    "    else train_data.iloc[:, 0]\n",
    ")\n",
    "\n",
    "print('Загрузка валидационного датасета...')\n",
    "val_data = pd.read_csv(f'{data_dir}val.csv')\n",
    "val_texts = (\n",
    "    val_data['tweet'] if 'tweet' in val_data.columns\n",
    "    else val_data.iloc[:, 0]\n",
    ")\n",
    "\n",
    "print('Загрузка тестового датасета...')\n",
    "test_data = pd.read_csv(f'{data_dir}test.csv')\n",
    "texts = test_data['tweet'].dropna().tolist()\n",
    "selected_texts = random.sample(texts, 10)\n",
    "\n",
    "# Создаем Dataloader'ы\n",
    "print('Создание даталоадеров...')\n",
    "train_dataloader = create_dataloader(train_texts, tokenizer)\n",
    "val_dataloader = create_dataloader(val_texts, tokenizer)\n",
    "\n",
    "# Определяем устройство\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Используемое устройство: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7305cf3",
   "metadata": {},
   "source": [
    "Запустим процесс обучения модели с периодическим измерением метрик на валидационной выборке (после каждой эпохи) и сохранением показателей в память TrainHistory для постраения графиков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0429e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем модель\n",
    "vocab_size = len(tokenizer)\n",
    "model = SimpleLSTM(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=128,\n",
    "    hidden_dim=256,\n",
    "    num_layers=2\n",
    ")\n",
    "\n",
    "print(f\"\\nМодель создана:\")\n",
    "print(f\"  Параметров: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"  Размер словаря: {vocab_size}\")\n",
    "\n",
    "# Обучаем модель\n",
    "trained_model, history = train_model_with_rouge(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    selected_texts=selected_texts,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    num_epochs=5,\n",
    "    learning_rate=0.001,\n",
    "    eval_every=50,  # Каждые 50 шагов\n",
    "    save_dir=save_directory,\n",
    ")\n",
    "\n",
    "# Визуализируем историю обучения\n",
    "history.plot_training_history()\n",
    "\n",
    "# Финальная оценка\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Финальная оценка модели:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "prompts = []\n",
    "references = []\n",
    "for text in selected_texts:\n",
    "    split_data = split_text_3_4(text, tokenizer)\n",
    "    prompts.append(split_data['prompt'])\n",
    "    references.append(split_data['target'])\n",
    "\n",
    "results = generate_and_evaluate(\n",
    "    model, tokenizer, prompts, references, device,\n",
    ")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
